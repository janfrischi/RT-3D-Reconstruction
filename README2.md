### **README.md**

---

# **Real-Time Vision Pipeline for 3D Object Detection, Segmentation, and Reconstruction**

This repository implements a modular, real-time vision pipeline for object detection, segmentation, and 3D reconstruction using **YOLOv11** and **ZED Stereo Cameras**. The project leverages GPU acceleration and advanced computer vision techniques to create a scalable solution for collaborative robotics, workspace analysis, and dynamic object tracking.

---

## **Key Features**

### **1. Real-Time Object Detection/Segmentation and Tracking**
- Integrates **YOLOv11** to detect and segment objects of interest (e.g., bottles, cups, laptops) from stereo camera frames.
- Tracking ensures persistence of object IDs across frames.

### **2. Depth Map Conversion to 3D Points**
- Converts 2D segmentation masks into 3D point clouds using depth maps generated by ZED stereo cameras.
- Employs camera intrinsics for accurate depth-to-3D mapping.

### **3. Point Cloud Processing**
- **Downsampling**: Reduces point cloud density using voxel grid filtering for efficient processing.
- **Outlier Removal**: Removes statistical outliers to improve point cloud quality.
- **Fusion**: Merges point clouds from two cameras based on centroid distances to create a comprehensive 3D model.

### **4. Display and Visualization**
- Annotated video frames display bounding boxes, segmentation masks, and object labels in real time.
- Visualizes 3D point clouds of detected objects and workspace using Open3D.
- Displays frame rate (FPS) directly on the annotated frames for performance monitoring.

### **5. Modular Architecture**
- **Extensible Components**:
  - Camera management
  - Object detection and segmentation
  - Point cloud generation and fusion
- Each class and function has specific responsibilities, ensuring ease of integration and customization.

---

## **Key Components and Functionalities**

1. **Real-Time Object Detection and Tracking**:
   - **YOLOv11** is used to process frames from both cameras and detect specific objects in real time.

2. **3D Reconstruction**:
   - Depth information from ZED cameras is converted to 3D point clouds.
   - Downsampling and statistical outlier removal improve accuracy and processing speed.

3. **Point Cloud Fusion**:
   - Merges point clouds from both cameras to provide a unified 3D representation.
   - Centroid-based matching ensures accurate fusion.

4. **Performance Monitoring**:
   - Real-time FPS tracking is logged to a CSV file for benchmarking.
   - Each stage of the pipeline logs timings for analysis and optimization.

5. **Interactive Visualization**:
   - Annotated frames with object labels and bounding boxes.
   - 3D visualizations of fused point clouds, displayed interactively.

---

## **Getting Started**

### **Prerequisites**
1. **Hardware**:
   - NVIDIA RTX 4090 or equivalent GPU.
   - ZED Stereo Cameras.

2. **Software**:
   - Python >= 3.8
   - CUDA-enabled GPU drivers.

### **Installation**
1. Clone the repository:
   ```bash
   git clone https://github.com/username/vision-pipeline.git
   cd vision-pipeline
   ```

2. Install dependencies:
   ```bash
   pip install numpy pyzed-python opencv-python torch ultralytics open3d
   ```

3. Download the YOLOv11 model:
   - Ensure the YOLOv11 model (`yolo11x-seg.pt`) is stored in the `models/` directory.
   - You can download it from the [Ultralytics repository](https://github.com/ultralytics).

4. Connect ZED cameras and set up the environment.

---

## **Usage**

### **Running the Pipeline**
Run the main script to start the real-time pipeline:
```bash
python main.py
```

### **Expected Output**
1. **Real-Time Detection and Annotation**:
   - Frames from both cameras are displayed with bounding boxes, labels, and FPS overlay.
2. **3D Visualization**:
   - Fused 3D point clouds are displayed in an Open3D interactive window.

---

## **Repository Structure**

```
vision-pipeline/
├── main.py                     # Entry point for the pipeline.
├── models/                     # Directory for YOLOv11 model files.
├── utils/
│   ├── vision_pipeline_utils.py  # Helper functions for point cloud and mask processing.
├── requirements.txt            # List of dependencies.
├── fps_log.csv                 # Logs FPS data for benchmarking.
└── timings.csv                 # Logs timings for each pipeline step.
```

---

## **Performance Benchmarking**

The pipeline tracks the following metrics for each frame:
- **Frame Retrieval Time**: Time taken to retrieve frames from cameras.
- **Depth Retrieval Time**: Time for generating depth maps.
- **YOLO Inference Time**: Time for object detection and segmentation.
- **Point Cloud Processing Time**: Time to process and fuse point clouds.
- **Overall Loop Time**: Total time per frame.

All timings are logged in `timings.csv` for analysis.

---

## **Core Functions**

1. **`convert_mask_to_3d_points()`**: Converts 2D segmentation masks into 3D coordinates using depth maps.
2. **`downsample_point_cloud_gpu()`**: Downsamples point clouds using voxel grid filtering on the GPU.
3. **`crop_point_cloud_gpu()`**: Crops point clouds to specified boundaries.
4. **`fuse_point_clouds_centroid()`**: Fuses point clouds from two cameras based on centroid distance.
5. **`subtract_point_clouds_gpu()`**: Subtracts object point clouds from the workspace.

For a detailed description of functions, see the **[Functions Documentation](#core-functions)** section.

---

## **Notes**
- The code is designed to run in real-time, with optimizations for using CUDA if available.
- The application runs in a loop until the `q` key is pressed.

---

## **Troubleshooting**

### **Common Issues and Solutions**:

1. **Camera Initialization Error**:
   - Ensure the correct serial numbers are provided for your ZED cameras in the code.

2. **CUDA Errors**:
   - Verify your GPU drivers and CUDA toolkit are properly installed.
   - Ensure PyTorch is installed with CUDA support.

3. **Model Not Found**:
   - Ensure the YOLO model file is stored at the specified path (`models/yolo11x-seg.pt`).

---

## **Future Improvements**

1. **Dynamic Object Tracking**:
   - Add motion prediction for objects moving between frames.
2. **Occlusion Handling**:
   - Improve 3D reconstruction by compensating for occluded regions.
3. **Model Enhancement**:
   - Support additional object classes and improve segmentation accuracy.
4. **Extended Applications**:
   - Adapt the pipeline for AR/VR, autonomous robotics, and collaborative environments.

---

## **Contributing**

Contributions are welcome! To contribute:
1. Fork the repository.
2. Create a feature branch:
   ```bash
   git checkout -b feature-name
   ```
3. Commit your changes:
   ```bash
   git commit -m "Add new feature"
   ```
4. Push your branch and create a pull request.

---

## **License**

This project is licensed under the [MIT License](LICENSE).

---

## **Contact**

Feel free to reach out for questions or collaborations:
- **Name**: Jan Frischknecht
- **Email**: your-email@example.com
- **LinkedIn**: [LinkedIn Profile](https://linkedin.com/in/jan-frischknecht)

---